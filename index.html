<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Splash by zhangyuc</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Splash</h1>
      <h2 class="project-tagline">Parallelizing Stochastic Learning on Spark</h2>
      <a href="https://github.com/zhangyuc/splash" class="btn">View on GitHub</a>
      <a href="https://github.com/zhangyuc/splash/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/zhangyuc/splash/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="introducing-splash" class="anchor" href="#introducing-splash" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introducing Splash</h1>

<p><strong>Stochastic algorithm</strong> processes a large-scale dataset by sequentially processing its random samples. Typical examples of stochastic algorithm include <strong>stochastic gradient descent</strong>, <strong>Gibbs sampling</strong> and <strong>stochastic variational inference</strong>. Stochastic algorithms are often order-of-magnitude faster than batch processing algorithms. Below is a comparison of parallel Gradient Descent and SGD:</p>

<p align="center">
<img src="https://raw.githubusercontent.com/zhangyuc/splash/gh-pages/images/sgd.png" width="350"><br>
</p>

<p><strong>Splash</strong> is a user-friendly programming interface for developing and running stochastic algorithms on distributed systems. The main features of Splash are:</p>

<ul>
<li><p><strong>Easy programming</strong>: User develop single-thread algorithms via Splash: no communication protocol, no conflict management, no data partitioning, no hyper-parameter tunning.</p></li>
<li><p><strong>Fast Performance</strong>: Splash adopts novel strategies for automatic parallelization, so that inter-machine communication is no longer a performance bottleneck. Splash allows lightening-fast learning on commercial distributed systems such as Amazon EC2. Below is the running time of Splash and other algorithms for achieving the same learning accuracy:</p></li>
</ul>

<p align="center">
<img src="https://raw.githubusercontent.com/zhangyuc/splash/gh-pages/images/bar-sgd.png" width="350">
<img src="https://raw.githubusercontent.com/zhangyuc/splash/gh-pages/images/bar-svi.png" width="350"><br>
</p>

<ul>
<li>
<strong>Integration with Apache Spark</strong>: Splash is built on <a href="https://spark.apache.org/">Apache Spark</a>. it takes the resilient distributed dataset (RDD) of Spark as input and generates RDD as output. It works seamlessly with other data analysis tools in the Spark environment, including the <a href="https://spark.apache.org/mllib/">MLlib machine learning library</a>. </li>
</ul>

<h1>
<a id="install-splash-v001" class="anchor" href="#install-splash-v001" aria-hidden="true"><span class="octicon octicon-link"></span></a>Install Splash v.0.0.1</h1>

<p>To install Splash, you need to:</p>

<ol>
<li>Download and install <a href="http://www.scala-lang.org/index.html">scala</a>, <a href="http://www.scala-sbt.org/index.html">sbt</a> and <a href="https://spark.apache.org/">Apache Spark</a>.</li>
<li>Download the <a href="https://github.com/zhangyuc/splash/blob/master/target/scala-2.10/splash_2.10-0.0.1.jar?raw=true">Splash jar file</a> and put it in your project classpath.</li>
<li>Make the Splash jar file as a dependency when <a href="http://spark.apache.org/docs/1.2.1/submitting-applications.html">submitting Spark jobs</a>.</li>
</ol>

<h1>
<a id="quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span class="octicon octicon-link"></span></a>Quick Start</h1>

<h2>
<a id="import-splash" class="anchor" href="#import-splash" aria-hidden="true"><span class="octicon octicon-link"></span></a>Import Splash</h2>

<p>When Splash is in your project classpath, you can write a self-contained application using the Splash API. Besides importing Spark packages, you also need to import the Splash package in scala, by typing:</p>

<div class="highlight highlight-scala"><pre><span class="pl-k">import</span> <span class="pl-v">splash.</span><span class="pl-v">_</span></pre></div>

<h2>
<a id="create-dataset" class="anchor" href="#create-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Create Dataset</h2>

<p>Just as every Spark applications, the first step is to create a dataset for the learning algorithm. Splash provides an abstraction called <strong>parametrized RDD</strong>. The parametrized RDD is very similar to the resilient distributed dataset (RDD) of Spark, but it has particular data structure for maintaining the parameters to be learnt. A parametrized RDD can be created by a standard RDD:</p>

<div class="highlight highlight-scala"><pre><span class="pl-k">val</span> <span class="pl-en">paramRdd</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">ParametrizedRDD</span>(data)</pre></div>

<p>where <code>data</code> is a standard RDD object containing your dataset. </p>

<h2>
<a id="define-data-processing-function" class="anchor" href="#define-data-processing-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>Define Data Processing Function</h2>

<p>To execute the learning algorithm on the dataset, set a data processing function <code>process</code> to the parametrized RDD by</p>

<div class="highlight highlight-scala"><pre>paramRdd.setProcessFunction(process)</pre></div>

<p>The <code>process</code> function is implemented by the user. It takes four objects as input: a random sample from the dataset, the weight of this sample, the set of shared variables used by the stochastic algorithm and the set of local variables associated with the data element. Its functionality is to process the input to perform some update on the shared variables or on the local variables. The <code>process</code> function should be able to process a sequence of weighted samples. <strong>The capability of processing weighted samples is mandatory</strong> even if the original data is unweighted. This is because that Splash will automatically assign non-trivial weights to random samples as a part of its parallelization strategy. An exemplary implementation for <strong>logistic regression</strong> is:</p>

<div class="highlight highlight-scala"><pre><span class="pl-k">val</span> <span class="pl-en">process</span> <span class="pl-k">=</span> (<span class="pl-v">element</span>: (<span class="pl-k">Int</span>, <span class="pl-en">Array</span>[<span class="pl-k">Int</span>], <span class="pl-en">Array</span>[<span class="pl-k">Double</span>]), <span class="pl-v">weight</span>: <span class="pl-k">Double</span>, <span class="pl-v">sharedVar</span> : <span class="pl-en">SharedVariableSet</span>,  <span class="pl-v">localVar</span>: <span class="pl-en">LocalVariableSet</span>) <span class="pl-k">=&gt;</span> {
  <span class="pl-k">val</span> <span class="pl-en">y</span> <span class="pl-k">=</span> element._1
  <span class="pl-k">val</span> <span class="pl-en">x_key</span> <span class="pl-k">=</span> element._2
  <span class="pl-k">val</span> <span class="pl-en">x_value</span> <span class="pl-k">=</span> element._3

  <span class="pl-k">val</span> <span class="pl-en">t</span> <span class="pl-k">=</span> sharedVar.get(<span class="pl-s"><span class="pl-pds">"</span>t<span class="pl-pds">"</span></span>)
  <span class="pl-k">val</span> <span class="pl-en">learningRate</span> <span class="pl-k">=</span> sharedVar.get(<span class="pl-s"><span class="pl-pds">"</span>learning.rate<span class="pl-pds">"</span></span>)
  <span class="pl-k">var</span> <span class="pl-en">sum</span> <span class="pl-k">=</span> <span class="pl-c1">0.0</span>
  <span class="pl-k">for</span>(i <span class="pl-k">&lt;</span><span class="pl-k">-</span> <span class="pl-c1">0</span> until x_key.length){
    <span class="pl-c">// get the shared variable value by key "w:"+x_key(i)</span>
    sum <span class="pl-k">+</span><span class="pl-k">=</span> sharedVar.get(<span class="pl-s"><span class="pl-pds">"</span>w:<span class="pl-pds">"</span></span><span class="pl-k">+</span>x_key(i)) <span class="pl-k">*</span> x_value(i)
  }

  <span class="pl-k">for</span>(i <span class="pl-k">&lt;</span><span class="pl-k">-</span> <span class="pl-c1">0</span> until x_key.length)
  {
    <span class="pl-k">val</span> <span class="pl-en">delta</span> <span class="pl-k">=</span> weight <span class="pl-k">*</span> learningRate <span class="pl-k">/</span> math.sqrt( t <span class="pl-k">+</span> <span class="pl-c1">1</span> ) <span class="pl-k">*</span> y <span class="pl-k">/</span> (<span class="pl-c1">1.0</span> <span class="pl-k">+</span> math.exp(y<span class="pl-k">*</span>sum)) <span class="pl-k">*</span> x_value(i)
    <span class="pl-c">// increase the shared variable by delta</span>
    sharedVar.add(<span class="pl-s"><span class="pl-pds">"</span>w:<span class="pl-pds">"</span></span> <span class="pl-k">+</span> x_key(i), delta)
  }
  <span class="pl-c">// increase the shared variable by weight</span>
  sharedVar.add(<span class="pl-s"><span class="pl-pds">"</span>t<span class="pl-pds">"</span></span>, weight)
}</pre></div>

<p>in the above code, the data element <code>element</code> has three components: a binary label equal to -1 or 1, an array of feature indices and an array of feature values. The line of code</p>

<div class="highlight highlight-scala"><pre>sum <span class="pl-k">+</span><span class="pl-k">=</span> sharedVar.get(<span class="pl-s"><span class="pl-pds">"</span>w:<span class="pl-pds">"</span></span><span class="pl-k">+</span>x_key(i)) <span class="pl-k">*</span> x_value(i)</pre></div>

<p>reads the weight associated with the feature index <code>x_key(i)</code>. These weights are used to compute a linear combination of feature values, and compute the gradient of the logistic loss. Then, the shared vector is updated by an <code>add</code> operator, taking a small step towards the opposite direction of the gradient. We make the stepsize proportional to <code>weight</code>. This is the simplest way of processing weighted samples. The related codes are:</p>

<div class="highlight highlight-scala"><pre><span class="pl-k">val</span> <span class="pl-en">delta</span> <span class="pl-k">=</span> weight <span class="pl-k">*</span> learningRate <span class="pl-k">/</span> math.sqrt( t <span class="pl-k">+</span> <span class="pl-c1">1</span> ) <span class="pl-k">*</span> y  <span class="pl-k">/</span> (<span class="pl-c1">1.0</span> <span class="pl-k">+</span> math.exp(y<span class="pl-k">*</span>sum)) <span class="pl-k">*</span> x_value(i)
sharedVar.add(<span class="pl-s"><span class="pl-pds">"</span>w:<span class="pl-pds">"</span></span> <span class="pl-k">+</span> x_key(i), delta)</pre></div>

<p>There is another shared variable <code>t</code> counting the number of elements. It controls the stepsize of stochastic gradient descent, ensuring it decreasing to zero as more samples are processed. The sample weight is added to the counter after the sample is processed. Similarly, it is updated by calling the <code>get</code> operator and the <code>add</code> operator.</p>

<p>More generally, the value of local/shared variables are accessed by the <code>get</code> operator:</p>

<div class="highlight highlight-scala"><pre><span class="pl-k">val</span> <span class="pl-en">v1</span> <span class="pl-k">=</span> localVar.get(key)
<span class="pl-k">val</span> <span class="pl-en">v2</span> <span class="pl-k">=</span> sharedVar.get(key)</pre></div>

<p>Local variables are updated by directly putting a new value to the key:</p>

<div class="highlight highlight-scala"><pre>localVar.set(key,value)</pre></div>

<p>while shared variables can be updated by putting the incremental change:</p>

<div class="highlight highlight-scala"><pre>sharedVar.add(key,delta)</pre></div>

<p>where <code>delta</code> is the difference between the new value and the old value. Besides the <code>add</code> operator, Splash  provides many flexible ways to manipulate the shared variable. There are multiple supported types of shared variables as well. See the Splash API for more options.</p>

<h2>
<a id="running-the-algorithm" class="anchor" href="#running-the-algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>Running the Algorithm</h2>

<p>After setting up the processing function, the user calls <code>run</code> to start running the stochastic algorithm:</p>

<div class="highlight highlight-scala"><pre><span class="pl-k">val</span> <span class="pl-en">spc</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">StreamProcessContext</span>
paramRdd.run(spc)</pre></div>

<p>The stream process context <code>spc</code> provides interfaces to control the algorithm execution. The user can specify the number of parallel threads, the proportion of data to process per iteration, the strategy for combining parallel updates, etc. See the Splash API for more details. In this example, we use the system's default setting. In the default setting, the algorithm takes one pass over the dataset by one calling of <code>run</code>. In most machine learning applications, the user wants to call <code>run</code> multiple times to take multiple passes over the dataset, in order to obtain higher learning accuracy.</p>

<h2>
<a id="output-and-evaluation" class="anchor" href="#output-and-evaluation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Output and Evaluation</h2>

<p>After processing the data, the shared variable set can be accessed by:</p>

<div class="highlight highlight-scala"><pre><span class="pl-k">val</span> <span class="pl-en">sharedVar</span> <span class="pl-k">=</span> paramRdd.getFirstSharedVariable()</pre></div>

<p>which returns the shared variable set maintained by the first RDD partition. The user can also query the shared variable set from all RDD partitions by call <code>paramRdd.getAllSharedVariable()</code>. It is also possible to manipulate the parametrized RDD directly. For example, by calling</p>

<div class="highlight highlight-scala"><pre><span class="pl-k">val</span> <span class="pl-en">loss</span> <span class="pl-k">=</span> paramRdd.map(evaluateLoss).reduce( _ <span class="pl-k">+</span> _ )</pre></div>

<p>every element in the dataset is processed by the <code>evaluateLoss</code> function. The resulting losses are aggregated by the <code>reduce</code> operator. The <code>map</code> operator for parametrized RDD is different from the standard <code>map</code> in that it is granted access to not only a data element, but also the associated local and shared variables. As a concrete example, we define a function for evaluating the logistic loss:</p>

<div class="highlight highlight-scala"><pre><span class="pl-k">val</span> <span class="pl-en">evaluateLoss</span> <span class="pl-k">=</span> (<span class="pl-v">element</span>: (<span class="pl-k">Int</span>, <span class="pl-en">Array</span>[<span class="pl-k">String</span>], <span class="pl-en">Array</span>[<span class="pl-k">Double</span>]), <span class="pl-v">sharedVar</span> : <span class="pl-en">SharedVariableSet</span>,  <span class="pl-v">localVar</span>: <span class="pl-en">LocalVariableSet</span> ) <span class="pl-k">=&gt;</span> {
  <span class="pl-k">val</span> <span class="pl-en">y</span> <span class="pl-k">=</span> element._1
  <span class="pl-k">val</span> <span class="pl-en">x_key</span> <span class="pl-k">=</span> element._2
  <span class="pl-k">val</span> <span class="pl-en">x_value</span> <span class="pl-k">=</span> element._3

  <span class="pl-k">var</span> <span class="pl-en">sum</span> <span class="pl-k">=</span> <span class="pl-c1">0.0</span>
  <span class="pl-k">for</span>(i <span class="pl-k">&lt;</span><span class="pl-k">-</span> <span class="pl-c1">0</span> until x_key.length){
    sum <span class="pl-k">+</span><span class="pl-k">=</span> sharedVar.get(<span class="pl-s"><span class="pl-pds">"</span>w:<span class="pl-pds">"</span></span> <span class="pl-k">+</span> x_key(i)) <span class="pl-k">*</span> x_value(i)
  }
  math.log( <span class="pl-c1">1.0</span> <span class="pl-k">+</span> math.exp( <span class="pl-k">-</span> y <span class="pl-k">*</span> sum ) )
}</pre></div>

<p>It provides a convenient way to evaluate the performance of the algorithm. See Splash API for more options of manipulating the parametrized RDD.</p>

<h2>
<a id="try-logistic-regression-example" class="anchor" href="#try-logistic-regression-example" aria-hidden="true"><span class="octicon octicon-link"></span></a>Try Logistic Regression Example</h2>

<p>To run the logistic regression example (with stochastic gradient descent), download the <a href="https://github.com/zhangyuc/splash/blob/master/examples/SplashExample.tar.gz?raw=true">Splash Example package</a> and extract it at any directory. The Splash library is included in the package, so you don't have to download it again. To run the code, <code>cd</code> into that directory, then compile the code by typing:</p>

<pre><code>sbt package
</code></pre>

<p>It generates a jar file at <code>./target/scala-2.10/splashexample_2.10-1.0.jar</code>. We submit this jar file as a Spark job by typing</p>

<pre><code>*YOUR_SPARK_HOME*/bin/spark-submit --class *ExampleName* \
--driver-memory 4G \
--jars lib/splash_2.10-0.0.1.jar target/scala-2.10/splashexample_2.10-1.0.jar \
data/covtype.txt 20.0 &gt; output.txt
</code></pre>

<p>Here, <code>*YOUR_SPARK_HOME*</code> should be the directory that you have installed Spark. <code>*ExampleName*</code> indicates the name of the example. To run logistic regression, set <code>*ExampleName* = LogisticRegression</code>. The file <code>splash_2.10-0.0.1.jar</code> is the Splash library and <code>splashexample_2.10-1.0.jar</code> is the compiled code to be executed. The two arguments of the app are <code>data/covtype.txt</code> and <code>20.0</code>, which stand for the location of the data file and the learning rate. The result is output to <code>output.txt</code>.</p>

<p>After the algorithm terminates (it takes 100 passes over the dataset), the output should be like:</p>

<pre><code>Stochastic Gradient Descent
Dataset contains 581012 data points.
Time = 1.737; Loss = 0.51776285; Group Number = 8
Time = 2.718; Loss = 0.51625430; Group Number = 8
Time = 3.505; Loss = 0.51591477; Group Number = 8
...
</code></pre>

<p>Each line corresponds to the outcome after an additional pass over the dataset. The first two quantities are the total time elapsed and the average logistic loss. The last quantity is a system-chosen parameter called <em>Group Number</em>. It stands for the number of groups that parallel threads are clustered into. The parallelization strategy of Splash clusters the parallel threads into groups before aggregating their updates. Although this implementation detail is hidden from the user, the group number is helpful in monitoring the statues of the system.</p>

<p>There is another example in the package called <em>FasterLogisticRegression</em>. It is an alternative implementation of the logistic regression update using the <strong>array interface</strong>. In this implementation, the vector is stored in an array and the update is performed by manipulating array elements. Since manipulating array is more efficient than manipulating single key-value pairs, this implementation is faster than the previous example. The user is encouraged to verify the performance improvement on their own computer.</p>

<h1>
<a id="splash-api" class="anchor" href="#splash-api" aria-hidden="true"><span class="octicon octicon-link"></span></a>Splash API</h1>

<p>In this section, we provide a brief description on the Splash API. </p>

<h2>
<a id="parametrized-rdd-operators" class="anchor" href="#parametrized-rdd-operators" aria-hidden="true"><span class="octicon octicon-link"></span></a>Parametrized RDD Operators</h2>

<p>The parametrized RDD provides a similar set of operations that are supported by Spark RDD. Since the parametrized RDD maintains local variables and shared variables, there are additional operations manipulating these data structures.</p>

<table>
<thead>
<tr>
<th>Operation</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>this(<em>rdd</em>, <em>preservePartitions</em>)</td>
<td>Constructor. It returns a Parametrized RDD object constructed from <code>rdd</code>. There is an optional boolean argument <code>preservePartitions</code>. If <code>preservePartitions = true</code>, then the partitioning of <code>rdd</code> will be preserved. Otherwise, the original RDD will be repartitioned such that the number of partitions is equal to the number of available cores. The default value of <code>preservePartitions</code> is <code>false</code>.</td>
</tr>
<tr>
<td>map(<em>func</em>)</td>
<td>Return a RDD formed by mapping each element by function <code>func</code>. The function takes the element and the associated local/shared variables as input</td>
</tr>
<tr>
<td>foreach(<em>func</em>)</td>
<td>Process each element by function <code>func</code>. The function takes the element and the associated local/shared variables as input.</td>
</tr>
<tr>
<td>reduce(<em>func</em>)</td>
<td>Reduce all elements by function <code>func</code>. The function takes two elements as input and returns a single element as output.</td>
</tr>
<tr>
<td>mapSharedVariable (<em>func</em>)</td>
<td>Return a RDD formed by mapping the shared variable set by function <code>func</code>.</td>
</tr>
<tr>
<td>foreachSharedVariable (<em>func</em>)</td>
<td>Process the shared variable set by function <code>func</code>.</td>
</tr>
<tr>
<td>reduceSharedVariable(<em>func</em>)</td>
<td>Reduce all shared variable sets by function <code>func</code>. The function takes two SharedVariableSet objects as input and returns a single SharedVariableSet object as output.</td>
</tr>
<tr>
<td>syncSharedVariable()</td>
<td>Synchronize the shared variable across all partitions. This operation often follows the execution of the above four operations. If the shared variables is manually changed but not synchronized, the change may not actually take effect.</td>
</tr>
<tr>
<td>getFirstSharedVariable()</td>
<td>Return the set of shared variables in the first partition.</td>
</tr>
<tr>
<td>getAllSharedVariable()</td>
<td>Return an array of the set of shared variables in all partitions.</td>
</tr>
<tr>
<td>setProcessFunction (<em>func</em>)</td>
<td>Set the data processing function. The function <code>func</code> takes an arbitrary element, the weight of the element and the associated local/shared variables. It performs update on the local/shared variables.</td>
</tr>
<tr>
<td>setLossFunction(<em>func</em>)</td>
<td>Set a loss function for the stochastic algorithm. The function <code>func</code> takes an element and the associated local/shared variables. It returns the loss incurred by this element. Setting a loss function for the algorithm is optional, but a reasonable loss function may help Splash choosing a better parallelization strategy.</td>
</tr>
<tr>
<td>run(<em>spc</em>)</td>
<td>Use the data processing function to process the dataset. <code>spc</code> is a StreamProcessContext object. It includes hyper-parameters for running the algorithm.</td>
</tr>
<tr>
<td>duplicateAndReshuffle(<em>n</em>)</td>
<td>Make <code>n-1</code> copies of every element and reshuffle them across partitions. This will enlarge the dataset by a factor of <code>n</code>. Parallel threads can reduce communication costs by processing a larger local dataset. If <code>n=1</code>, then the dataset is simply reshuffled.</td>
</tr>
</tbody>
</table>

<h2>
<a id="shared-variable-set-operators" class="anchor" href="#shared-variable-set-operators" aria-hidden="true"><span class="octicon octicon-link"></span></a>Shared Variable Set Operators</h2>

<p>All shared variables are organized as a SharedVariableSet instance. There are operations for reading and writing the shared variable set. We list them in a table:</p>

<table>
<thead>
<tr>
<th>Operation</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>get(<em>key</em>)</td>
<td>Return the value of the key. The initial value is 0.</td>
</tr>
<tr>
<td>add(<em>key</em>, <em>delta</em>)</td>
<td>Add <code>delta</code> to the value of the key.</td>
</tr>
<tr>
<td>delayedAdd(<em>key</em>, <em>delta</em>)</td>
<td>Same as <code>add</code>, but the operation will not be executed instantly. Instead, it will be executed at the next time the same element is processed. The delayed operation is useful for reversing a previous operation on the same element, or for passing information to the future.</td>
</tr>
<tr>
<td>multiply(<em>key</em>, <em>gamma</em>)</td>
<td>Multiply the value of the key by <code>gamma</code>.</td>
</tr>
<tr>
<td>declareArray(<em>key</em>, <em>length</em>)</td>
<td>Declare an array associated with the <code>key</code>. The <code>length</code> argument indicates the dimension of the array. The array has to be declared before manipulated. Generally speaking, manipulating an array of real numbers is faster than manipulating the same number of key-value pairs.</td>
</tr>
<tr>
<td>getArray(<em>key</em>)</td>
<td>Return the array associated with the key. It will return <code>null</code> if the array has not been declared.</td>
</tr>
<tr>
<td>getArrayElement(<em>key</em>, <em>ind</em>)</td>
<td>Return the array element with index <code>ind</code>. It will return 0 if the array has not been declared.</td>
</tr>
<tr>
<td>addArray(<em>key</em>, <em>delta</em>)</td>
<td>Add an array <code>delta</code> to the array associated with the key.</td>
</tr>
<tr>
<td>addArrayElement (<em>key</em>, <em>ind</em>, <em>delta</em>)</td>
<td>Add <code>delta</code> to the array element with index <code>ind</code>.</td>
</tr>
<tr>
<td>delayedAddArray (<em>key</em>, <em>delta</em>)</td>
<td>The same as <code>addArray</code>, but the operation will not be executed until the next time the same element is processed.</td>
</tr>
<tr>
<td>delayedAddArrayElement (<em>key</em>, <em>delta</em>, <em>delta</em>)</td>
<td>The same as <code>addArrayElement</code>, but the operation will not be executed until the next time the same element is processed.</td>
</tr>
<tr>
<td>multiplyArray (<em>key</em>, <em>gamma</em>)</td>
<td>Multiply all elements of the array by a real number <code>gamma</code>. The computation complexity of this operation is <strong>O(1)</strong>, independent of the dimension of the array.</td>
</tr>
<tr>
<td>dontSync(<em>key</em>)</td>
<td>The system will not synchronize this variable at the end of the current iteration. It will improve the communication efficiency, but may cause unpredictable consistency issues. Don't register a variable as <code>dontSync</code> unless you are sure that it will never be used by other partitions. The variable will still be synchronized at later iterations.</td>
</tr>
<tr>
<td>dontSyncArray(<em>key</em>)</td>
<td>The same as <code>dontSync</code>, but the object is an array-type variable.</td>
</tr>
</tbody>
</table>

<h2>
<a id="stream-process-context" class="anchor" href="#stream-process-context" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stream Process Context</h2>

<p>The Stream Process Context allows the user setting customized properties for the algorithm execution. Given a Stream Process Context object <code>spc</code>, the properties are set by</p>

<div class="highlight highlight-scala"><pre>spc <span class="pl-k">=</span> spc.set(propertyName,propertyValue)</pre></div>

<p>Here is a list of configurable properties:</p>

<table>
<thead>
<tr>
<th>Property Name</th>
<th align="center">Default</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>num.of.thread</td>
<td align="center">0</td>
<td>The number of parallel threads for algorithm execution. If <code>num.of.thread = 0</code>, then the number of parallel thread is equal to the number of partitions.</td>
</tr>
<tr>
<td>num.of.group</td>
<td align="center">0</td>
<td>Indicate the number of groups that Splash clusters the parallel threads. It is a system-level parameter. If <code>num.of.group = 0</code>, then the group number will be automatically chosen.</td>
</tr>
<tr>
<td>num.of.pass.over .local.data</td>
<td align="center">1.0</td>
<td>Proportion of local data processed per one call of the <code>run</code> function. If this number is greater than 1, then each thread will take multiple passes over its local dataset before synchronization.</td>
</tr>
<tr>
<td>warmStart</td>
<td align="center">true</td>
<td>If <code>warmStart = true</code>, then at the first time the <code>run</code> function is called, the system will not execute parallel processing immediately. It will begin by running the algorithm sequentially on a small fraction of data as a warm start. This feature can be turned off by setting <code>warmStart = false</code>.</td>
</tr>
</tbody>
</table>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/zhangyuc/splash">Splash</a> is maintained by <a href="https://github.com/zhangyuc">zhangyuc</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>

